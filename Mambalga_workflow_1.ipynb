{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olympus-terminal/LLM-training/blob/main/Mambalga_workflow_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6d72dc7-d18f-40bc-8d6f-8d3130b4e043",
      "metadata": {
        "id": "b6d72dc7-d18f-40bc-8d6f-8d3130b4e043"
      },
      "source": [
        "We will fine-tune a pretrained MAMBA transformers-compatible LLM to recognize 'algal' or 'bacterial' signatures in protein sequence. First, we need to prepare the input data. If we check line sizes, we see that they are quite variable and could influence training through inconsistent padding lengths:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04f374d0-cfcb-404d-8433-87c86e44ab83",
      "metadata": {
        "id": "04f374d0-cfcb-404d-8433-87c86e44ab83"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "def print_line_char_counts(input_file):\n",
        "  \"\"\"\n",
        "  Prints the number of characters in each line of a text file.\n",
        "\n",
        "  Args:\n",
        "    input_file: The path to the input text file.\n",
        "  \"\"\"\n",
        "  with open(input_file, 'r') as infile:\n",
        "    for line_number, line in enumerate(infile, 1):\n",
        "      char_count = len(line.strip())\n",
        "      print(f\"Line {line_number}: {char_count} characters\")\n",
        "\n",
        "# Check if input file is provided as sys.argv[1]\n",
        "if len(sys.argv) > 1:\n",
        "  input_file = sys.argv[1]\n",
        "  print_line_char_counts(input_file)\n",
        "else:\n",
        "  print(\"Error: Please provide the input text file as a command-line argument.\")"
      ]
    },
    {
      "cell_type": "raw",
      "id": "87a12b1f-c8d6-4e38-945c-d8a285c2d042",
      "metadata": {
        "id": "87a12b1f-c8d6-4e38-945c-d8a285c2d042"
      },
      "source": [
        "Now we will remove line wraps and FASTA headers to get raw chunks of sequence data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2115cee2-b2c7-4a81-8af8-72e1022e7a20",
      "metadata": {
        "id": "2115cee2-b2c7-4a81-8af8-72e1022e7a20"
      },
      "outputs": [],
      "source": [
        "remove_wraps_and_headers.py\n",
        "import sys\n",
        "\n",
        "def remove_line_wrapping_and_headers(input_file):\n",
        "  \"\"\"\n",
        "  Removes line wrapping and headers from a FASTA file.\n",
        "\n",
        "  Args:\n",
        "    input_file: The path to the input FASTA file.\n",
        "  \"\"\"\n",
        "  output_file = f\"{input_file}.unwrapped_no_headers\"\n",
        "  with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
        "    for line in infile:\n",
        "      # Skip header lines and write only sequence data\n",
        "      if not line.startswith('>'):\n",
        "        outfile.write(line.strip())\n",
        "\n",
        "# Check if input file is provided as sys.argv[1]\n",
        "if len(sys.argv) > 1:\n",
        "  input_file = sys.argv[1]\n",
        "  remove_line_wrapping_and_headers(input_file)\n",
        "else:\n",
        "  print(\"Error: Please provide the input FASTA file as a command-line argument.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b47aedaa-95a0-4e28-8a52-942311b24fd3",
      "metadata": {
        "id": "b47aedaa-95a0-4e28-8a52-942311b24fd3"
      },
      "source": [
        "Now we will run the fine-tune and log the results at WandB. The main considerations for balancing performance and scalability are:\n",
        "1. Mamba model size: 130m, 370m, 790m, 1.4B, or 2.8B.\n",
        "2. The 'per_device_train_batch_size' parameter. This can speed up the job considerably and smooth out roughness but possibly blunt fine features and eventually cause OOM errors.\n",
        "3. Finally, 'the max_seq_length' parameter directly decreases the batch size needed to cause an OOM. F1 scores of ~80 when using short seq. lengths (128) but ~90 when using 2048."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2d0463b-6f02-4dbc-b581-733b46c2fb3b",
      "metadata": {
        "id": "c2d0463b-6f02-4dbc-b581-733b46c2fb3b"
      },
      "outputs": [],
      "source": [
        "####python archive_train_wandb-shuffle-downsample.py\n",
        "\n",
        "import wandb\n",
        "from datasets import Dataset\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
        "import pandas as pd\n",
        "import sys\n",
        "import zipfile\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "from einops import rearrange, repeat, einsum\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(project=\"MambalgaP\", entity=\"algaeai\")\n",
        "\n",
        "# Ensure there is a command-line argument provided\n",
        "if len(sys.argv) < 2:\n",
        "    print(\"Usage: python script.py <path_to_zip_file>\")\n",
        "    sys.exit(1)\n",
        "\n",
        "zip_file_path = sys.argv[1]\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-370m-hf\")\n",
        "#model = AutoModelForCausalLM.from_pretrained(\"state-spaces/mamba-370m-hf\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-1.4b-hf\", use_special_tokens=False)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"state-spaces/mamba-1.4b-hf\")\n",
        "\n",
        "# Function to extract zip file and read all text files\n",
        "def load_text_from_zip(zip_file_path, sample_size=None, fraction=1):\n",
        "    text_data = []\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"temp_data\")\n",
        "        for root, dirs, files in os.walk(\"temp_data\"):\n",
        "            for file in files:\n",
        "                if file.endswith(\".txt\"):\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                        text_data.extend(f.readlines())\n",
        "\n",
        "    # Shuffling the data\n",
        "    random.shuffle(text_data)\n",
        "\n",
        "    # Downsampling the data\n",
        "    if sample_size is not None:\n",
        "        text_data = text_data[:sample_size]\n",
        "    elif fraction is not None:\n",
        "        text_data = text_data[:int(len(text_data) * fraction)]\n",
        "\n",
        "    return {\"text\": [line.strip() for line in text_data if line.strip()]}\n",
        "\n",
        "# Load and preprocess the data\n",
        "data = load_text_from_zip(zip_file_path)\n",
        "dataframe = pd.DataFrame(data)\n",
        "dataset = Dataset.from_pandas(dataframe)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=28,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    learning_rate=5e-4,\n",
        "    report_to=\"wandb\"  # Integrate Weights & Biases for tracking\n",
        ")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    target_modules=[\"x_proj\", \"embeddings\", \"in_proj\", \"out_proj\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    peft_config=lora_config,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",  # Ensure this matches your data's text field\n",
        "    max_seq_length=128,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Clean up the extracted files\n",
        "os.system(\"rm -rf temp_data\")\n",
        "\n",
        "# Finish the wandb run\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "208affb0-c558-4c97-a6db-fe7d95c41f78",
      "metadata": {
        "id": "208affb0-c558-4c97-a6db-fe7d95c41f78"
      },
      "source": [
        "Now we deploy on an HPC cluster using 4 gpus per job (A100 or V100):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69839490-0554-44b6-9c9e-7fcebe22f486",
      "metadata": {
        "id": "69839490-0554-44b6-9c9e-7fcebe22f486"
      },
      "outputs": [],
      "source": [
        "#!/bin/bash\n",
        "\n",
        "#SBATCH --mem=800GB\n",
        "#SBATCH --time=96:00:00\n",
        "#SBATCH -p nvidia\n",
        "#SBATCH --gres=gpu:4\n",
        "#SBATCH --cpus-per-task=10\n",
        "\n",
        "PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True && python archive_train_wandb-shuffle-downsample.py /scratch/drn2/transfer/fasta-processing/finalize/training_100s.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The train run will save results as multiple checkpoint folders with .safetensors and .pkl files\n",
        "\n",
        "\n",
        "Now we will evaluate the fine-tuned model (Mambalga v.x) on a set of sequences (10% of total) not used for training:"
      ],
      "metadata": {
        "id": "slfFVXLeQFHB"
      },
      "id": "slfFVXLeQFHB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e26fb81-48ec-4d82-95cf-c8237e9f53fe",
      "metadata": {
        "id": "5e26fb81-48ec-4d82-95cf-c8237e9f53fe"
      },
      "outputs": [],
      "source": [
        "###filename: infer-singleLoad.py or infer-SL2(or 2).py\n",
        "\n",
        "import sys\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "def generate_output(input_text, model, tokenizer):\n",
        "    # Tokenize input text\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to('cuda')  # Move input_ids to GPU\n",
        "\n",
        "    # Generate output\n",
        "    with torch.no_grad():  # Disable gradient calculation for inference\n",
        "        outputs = model.generate(input_ids, max_new_tokens=12)\n",
        "\n",
        "    # Decode the output tokens back to text\n",
        "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "    return output_text\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if len(sys.argv) != 3:\n",
        "        print(\"Usage: python script.py <model_name_or_path> <input_file>\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    model_name_or_path = sys.argv[1]\n",
        "    input_file = sys.argv[2]\n",
        "\n",
        "    # Load the model and tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path).to('cuda')  # Move the model to GPU\n",
        "\n",
        "    with open(input_file, 'r') as file:\n",
        "        for line in file:\n",
        "            input_text = line.strip()\n",
        "            output_text = generate_output(input_text, model, tokenizer)\n",
        "            print(output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71e5dfed-329c-47fd-b43f-a6f3e95d79a9",
      "metadata": {
        "id": "71e5dfed-329c-47fd-b43f-a6f3e95d79a9"
      },
      "source": [
        "Lets again deploy on gpu nodes. This example is looking at the prediction results from a 1.4b training (0.04 epochs, or < 5% of the data [chkpt 355000]). The tuning is relatively slow (compared to the 130m model) because of the smaller batch size (16 compared to 512; necessary to not throw an OOM error) and because of the 10x increase in parameters. The WandB logs show this run to maintain a steadier grad_norm and better loss minimization, so we want to see if earlier checkpoints have any predictive power compared to smaller models.\n",
        "\n",
        "The input files are the 10% holdout sets from the algal and bacterial sequence blocks, and the output files contain the predictions we will use for evaluation of F1 scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2e93cb2-9f62-475f-9cbb-190941405ac0",
      "metadata": {
        "id": "d2e93cb2-9f62-475f-9cbb-190941405ac0"
      },
      "outputs": [],
      "source": [
        "#!/bin/bash\n",
        "\n",
        "#SBATCH -o slurm-logs/arrayJob_%A_%a.out\n",
        "#SBATCH -e slurm-logs/arrayJob_%A_%a.err\n",
        "#SBATCH -a 1-2\n",
        "#SBATCH --mem=250GB\n",
        "#SBATCH --time=4:00:00\n",
        "#SBATCH -p nvidia\n",
        "#SBATCH --gres=gpu:2\n",
        "#SBATCH --cpus-per-task=10\n",
        "\n",
        "\n",
        "LINE=$(sed -n \"$SLURM_ARRAY_TASK_ID\"p filelist.txt)\n",
        "echo \"$LINE\"\n",
        "\n",
        "python infer-singleLoad.py '/scratch/drn2/PROJECTS/AI/Mambalga/WDN_4/nest-afterEval/results/checkpoint-35500' $LINE > ./eval-results_\"$LINE\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ">>>The output looks like this:\n",
        "\n",
        "head eval-results_* ==> eval-results_bact1_accns.headers-fetched.aa.fa.unwrapped_no_headers.wrapped.10 <== EPTKREAQRIEKLQSKMQELAAAVDDALDAEDEDKADALQEEGEAVGEQLQALEDGLQDYSPTAKAAAGAIVTIDRNRQAVIHRGLMREAEAKALRTLER [<!!!>] [<!!!>] [<!!!>] [<!!!>] NNYVALHYEARHMNWLHSFWGLGASAGPAVMGLSLTLQWGYRNGYRVLGLMQTLQVIVLIASLALWTDPKGKRTAQDYPSQQKGTLRHKALPFALLSFFL [<!!!>] [<!!!>] [<!!!>] [<!!!>] VYKIIYQVLPEWWPKIAKRNMIVTGAAAGLAATFNTPLGGIVFAIEELTKTHFSYYKTAIFSSVIIAGLSAQALLGPYLYLGYPKLDGLTNTIFLGVAVV [<!!!>] [<!!!>] [<!!!>] [<!!!>] GIRQRQPLKIVVAVPVASPQAISNLQAEVDEVVCLYMPPAMGAVGYYYDEFAQVSDDDVVRLLAAFQKKNKANEVLSFLSENASESFSAKELSEKLNISE [<!!!>] [<!!!>] [<!!!>] [<!!!>] RVSAVADLAAFHACYTPALHRIAIIDLGLPDGEGMVLVRALRAQGHPIGIVVFTARGATQDKVNGLGGGADYYLPKSADLDELAATLGALARRLGAPPAD [<!!!>] [<!!!>] [<!!!>] [<!!!>] AFFGASTAFVESTLAQVYRFPHRSSFRGGPFCYIQEGLHKRWIGVLFAILTIVCYGVFLPTVQANGASQAFFNSYSVTPAATGIGLAILLAIVIIGGVKR [<!!!>] [<!!!>] [<!!!>] [<!!!>] YFGFMGEFGWGATWGEDSYDADEYDASSPDAWEGEERQIVLIANPGFRWRLGRSLFLNLGLYAGAAIDVKDEIVYINQNNTTEDYRGAMFFGMIEFALGW [<!!!>] [<!!!>] [<!!!>] [<!!!>] RITPAPSITKNELENWVRANASTDYHPCGTCRMGTDKHAVVDQELRVHGIDGLRIVDASVMPDILSGNLNAPTQMIAERAADYLMGRPQLPEEHAKFHFM [<!!!>] [<!!!>] [<!!!>] [<!!!>] LIDFMKKSCLTTFNDIDVTGKIKCSVDARLYYSNISEKTDFYNFKYFYDMKGNEIYVESADAGIKLNLLNMYNKPIHLWDLRGFHKKIHYDNLQRIKEVY [<!!!>] [<!!!>] [<!!!>] [<!!!>] RQLNPANKADRELAHSLGLATEEMLGSIARWSDDGLTSTHGKSEKLARISSGVASLVMRVSLLNALTAASKVGFTKLLMEKYGRLSRSKAWGDLDIQDRE [<!!!>] [<!!!>] [<!!!>] [<!!!>]\n",
        "\n",
        "==> eval-results_Filtered_algal_doubled.aa.fa.unwrapped_no_headers.wrapped.10 <== LARAADLSCDAWGHPCGPLHRMAHGHPLDPLLPRVCPIPAADSGSGTGAAPPRPPPSPSGGKGKGKKRPPPPAPAKDNRRPPSPAKKKKNRPPKNPKLGP [<@@@>] [<@@@>] [<@@@>] YMIDRNLPSPREMSRSRGSKFRAETPIFSLTGKNNLKVGESVELTAQEEMAKAESEMLRQTAKIERHRRQSRTIMVDSLTVTLEVPRPFCLVLLRQVCP [<@@@>] [<@@@>] [<@@@>] LDHMIGHIQYCRPAASVNLNPGSFLSQLVRMGAPSQLPPTHPPALSPAYSAAYSAACPHPHDPVHARALSSSCAPPHLMERGSRANWAGVAVTGAEAAF [<@@@>] [<@@@>] [< [< [< [< YPEGCMGLCRVHRQAQQDRKKGDAGSSVMLDISHLTPASLALHVVGCCRQLLLELVLTTALGPKAGELTSRRPDSRAAIGSARTRNLINYAKTTAAAAL [<@@@>] [<@@@>] [< [< [< [< GDDPTGGPTFKRADPTSQHDPSHQEVMGQDDVAASQAPAVPRGASESDLLGLGDTLLASRMALWTAGFVVMTLLINAPLLTPLMTLLGLNKATPGQLHV [<@@@>] [<@@@>] [< [< [< [< PTPHKLRMPLKLSRETVAEVKPASTASCSRLASQHGPGLGQLQLELNHRSQVRLTVNRQAGLKVPGDPRLRSTCCVLLTCSSSFCWASRATSARASTYYA [<@@@>] [<@@@>] [< [< [< [< QRKLVTLYHCPTTEMPADMLTKPLGRVLFEKFVGMVGLSECKAAQSMMDDQSSGSVRIEKLTASNFYIWKQKIQLLLALRDVDQYGFDDIPENATSDDRL [<@@@>] [<@@@>] [< [< [< GCAASNVGNGANAGYPCPPCYFGVTATRATHSSPVLSFRQPKESLVRLKELGGKVAGGVGQVVDKVRQKWDARQAHKKQNPPGGRGKVGRKQNKAQGNEG [<@@@>] [<@@@>] [<@@@>] KVGCHDHFHQRPCFEMHHLTCLACYSAPWGCFTKWGRWKCWRALLLAIRHKPMLHCSLAHLAQAEAINQLQEERAQLSTAVEQGLADQADLHTLRQQAG [<@@@>] [<@@@>] [< [< [< [< HQHQWLAFLDVDEFIVIDEEKVSIPLMLSNYKQYGGVGLNWVMFGSSGYDQRPEGGVRNYTMCNRSLDFTSLTAQSAHDPWRQCDHIEHNRQSNFPDNRH [<@@@>] [<@@@>] [<@@@>]\n"
      ],
      "metadata": {
        "id": "ZQHV5CIjQp1j"
      },
      "id": "ZQHV5CIjQp1j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b0577612-e7c3-4e90-9d66-0119c16a72e6",
      "metadata": {
        "id": "b0577612-e7c3-4e90-9d66-0119c16a72e6"
      },
      "source": [
        "From a glance, the model appears to be predicting sequence sources correctly. The generation of [<@@@>] indicates the model correctly assigned an algal tag to an algal input sequence, while lines with [<!!!>] signify bacterial IDs. There may be an emergent property of this method of NTG; bacterial sequences always get 4 tags, while algal seqs get either 2 or 3 tags. This may indicate that bacterial sequences are thoroughly such, while algal sequences vary in their degree of intrinsic 'algal-ness'. Seeing that algal lineages are relatively evolutionarily new compared to bacterial, the variance observed here might be giving some information on the sequences' evolutionary age. Defined sequence motifs, possibly quantified through SHAP, could indicate an 'old/core' algal seq, while others could indicate specialization or recent acquisitions by some lineages.\n",
        "\n",
        "Full inference of ~60,000 sequences (comprising the eval 10% holdout sets; can use 10,000 eval seqs for quickchecks) will take 3-3.5 hours on two gpus, so lets run the summary script to get stats on a previous run for an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25acbfaf-058d-4532-b35e-7fcd4ee99820",
      "metadata": {
        "id": "25acbfaf-058d-4532-b35e-7fcd4ee99820"
      },
      "outputs": [],
      "source": [
        "###filename: MoreMetrics.sh\n",
        "\n",
        "#!/bin/bash\n",
        "\n",
        "# Define color codes\n",
        "RED='\\033[0;31m'\n",
        "GREEN='\\033[0;32m'\n",
        "YELLOW='\\033[1;33m'\n",
        "NC='\\033[0m' # No Color\n",
        "\n",
        "# Counting algal hits\n",
        "echo -e \"${GREEN}Counting algal hits...${NC}\"\n",
        "fgrep -c '@' eval-resu*\n",
        "\n",
        "echo -e \"\\n${GREEN}Counting bacterial hits...${NC}\"\n",
        "fgrep -c '!' eval-resu*\n",
        "\n",
        "echo -e \"\\n${YELLOW}In other words, from the algal holdout set there are:${NC}\"\n",
        "count_algae=$(fgrep -c '@' eval-results_Filtered_algal_doubled.aa.fa.unwrapped_no_headers.wrapped.10)\n",
        "echo -e \"${GREEN}${count_algae} algal signatures.${NC}\"\n",
        "\n",
        "count_bacteria=$(fgrep -c '!' eval-results_Filtered_algal_doubled.aa.fa.unwrapped_no_headers.wrapped.10)\n",
        "echo -e \"${RED}${count_bacteria} bacterial signatures.${NC}\"\n",
        "\n",
        "echo -e \"\\n${YELLOW}And from the bacterial holdout set, there are:${NC}\"\n",
        "count_algae_bact=$(fgrep -c '@' eval-results_bact1_accns.headers-fetched.aa.fa.unwrapped_no_headers.wrapped.10)\n",
        "echo -e \"${GREEN}${count_algae_bact} algal signatures.${NC}\"\n",
        "\n",
        "count_bacteria_bact=$(fgrep -c '!' eval-results_bact1_accns.headers-fetched.aa.fa.unwrapped_no_headers.wrapped.10)\n",
        "echo -e \"${RED}${count_bacteria_bact} bacterial signatures.${NC}\"\n",
        "\n",
        "# Calculate performance metrics\n",
        "total_algae=$((count_algae + count_algae_bact))\n",
        "total_bacteria=$((count_bacteria + count_bacteria_bact))\n",
        "\n",
        "true_positives_algae=$count_algae\n",
        "false_positives_algae=$count_bacteria\n",
        "true_negatives_bacteria=$count_bacteria_bact\n",
        "false_negatives_bacteria=$count_algae_bact\n",
        "\n",
        "precision_algae=$(echo \"scale=4; $true_positives_algae / ($true_positives_algae + $false_positives_algae)\" | bc)\n",
        "recall_algae=$(echo \"scale=4; $true_positives_algae / ($true_positives_algae + $false_negatives_bacteria)\" | bc)\n",
        "f1_score_algae=$(echo \"scale=4; 2 * ($precision_algae * $recall_algae) / ($precision_algae + $recall_algae)\" | bc)\n",
        "\n",
        "precision_bacteria=$(echo \"scale=4; $true_negatives_bacteria / ($true_negatives_bacteria + $false_negatives_bacteria)\" | bc)\n",
        "recall_bacteria=$(echo \"scale=4; $true_negatives_bacteria / ($true_negatives_bacteria + $false_positives_algae)\" | bc)\n",
        "f1_score_bacteria=$(echo \"scale=4; 2 * ($precision_bacteria * $recall_bacteria) / ($precision_bacteria + $recall_bacteria)\" | bc)\n",
        "\n",
        "echo -e \"\\n${YELLOW}Performance Metrics:${NC}\"\n",
        "echo -e \"Algal Precision: ${GREEN}$precision_algae${NC}\"\n",
        "echo -e \"Algal Recall: ${GREEN}$recall_algae${NC}\"\n",
        "echo -e \"Algal F1 Score: ${GREEN}$f1_score_algae${NC}\"\n",
        "\n",
        "echo -e \"Bacterial Precision: ${RED}$precision_bacteria${NC}\"\n",
        "echo -e \"Bacterial Recall: ${RED}$recall_bacteria${NC}\"\n",
        "echo -e \"Bacterial F1 Score: ${RED}$f1_score_bacteria${NC}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98a2b08c-1ae0-45d0-96f3-6b3e7634989a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "98a2b08c-1ae0-45d0-96f3-6b3e7634989a",
        "outputId": "3c23ce9c-553c-4efb-ae03-e895e0a4fd5e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-1-52a649b84446>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-52a649b84446>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    <<OUTPUT>>>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "<<OUTPUT>>>\n",
        "\n",
        "Counting algal hits...\n",
        "eval-results_bact1_accns.headers-fetched.aa.fa.unwrapped_no_headers.wrapped.10:2910\n",
        "eval-results_Filtered_algal_doubled.aa.fa.unwrapped_no_headers.wrapped.10:27094\n",
        "\n",
        "Counting bacterial hits...\n",
        "eval-results_bact1_accns.headers-fetched.aa.fa.unwrapped_no_headers.wrapped.10:28731\n",
        "eval-results_Filtered_algal_doubled.aa.fa.unwrapped_no_headers.wrapped.10:5493\n",
        "\n",
        "In other words, from the algal holdout set there are:\n",
        "27094 algal signatures.\n",
        "5493 bacterial signatures.\n",
        "\n",
        "And from the bacterial holdout set, there are:\n",
        "2910 algal signatures.\n",
        "28731 bacterial signatures.\n",
        "\n",
        "Performance Metrics:\n",
        "Algal Precision: .8314\n",
        "Algal Recall: .9030\n",
        "Algal F1 Score: .8656\n",
        "Bacterial Precision: .9080\n",
        "Bacterial Recall: .8394\n",
        "Bacterial F1 Score: .8722"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This performance is OK, but we want to boost F1 scores. For one, we don't want any false positives showing as 'algae' from the bacterial set. Let's use a lower learning rate, larger max_seq_length, and we will use the 370m model. Since we will be benchmarking often, lets also sample the eval set to only 10,000 seqs for now."
      ],
      "metadata": {
        "id": "f9cwqvA2DTnU"
      },
      "id": "f9cwqvA2DTnU"
    },
    {
      "cell_type": "code",
      "source": [
        "C<<OUTPUT>>>\n",
        "\n",
        "ounting algal hits...\n",
        "eval-results_AlgalTop10000-10holdout:8299\n",
        "eval-results_BactTop10000-10holdout:723\n",
        "\n",
        "Counting bacterial hits...\n",
        "eval-results_AlgalTop10000-10holdout:1703\n",
        "eval-results_BactTop10000-10holdout:9277\n",
        "\n",
        "In other words, from the algal holdout set there are:\n",
        "8299 algal signatures.\n",
        "1703 bacterial signatures.\n",
        "\n",
        "And from the bacterial holdout set, there are:\n",
        "723 algal signatures.\n",
        "9277 bacterial signatures.\n",
        "\n",
        "Performance Metrics:\n",
        "Algal Precision: .8297\n",
        "Algal Recall: .9198\n",
        "Algal F1 Score: .8723\n",
        "Bacterial Precision: .9277\n",
        "Bacterial Recall: .8448\n",
        "Bacterial F1 Score: .8842"
      ],
      "metadata": {
        "id": "PvkTckT4EMvv"
      },
      "id": "PvkTckT4EMvv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These results are promising: there are less than half the 'false positives' from the bacterial set, as expected. Still, only marginal improvements on F1 scores were made. Lets increase the lora_alpha (default = 16) to 64; this will amplify the influence from the tagged amino acid sequences and dampen the original pretrained weights. We also know that adding math instructions, such as from the OpenOrca math training datasets, can improve reasoning. Lets add that to the training set to see if it helps as well.\n"
      ],
      "metadata": {
        "id": "U9ee9sKnEr_5"
      },
      "id": "U9ee9sKnEr_5"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}